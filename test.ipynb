{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Магистратура\\ВКР\\.venv\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.CenterCrop(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=10, p=0.2),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Для валидации/теста (только resize + norm)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.CenterCrop(224, 224),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "class CsvImageDataset(Dataset):\n",
    "    def __init__(self, csv_path, split, transform=None, class_names=None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        if class_names is None:\n",
    "            self.class_names = sorted(self.df['class_name'].unique())\n",
    "        else:\n",
    "            self.class_names = class_names\n",
    "\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.class_names)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = np.array(Image.open(row['image_path']).convert('RGB'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        label = self.class_to_idx[row['class_name']]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Конфиги ===\n",
    "CSV_PATH = 'merged_dataset_v2.csv'\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 0\n",
    "NUM_EPOCHS = 30\n",
    "LR = 1e-6\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = 'ViT-B-32'\n",
    "PRETRAINED = 'laion2b_s34b_b79k'\n",
    "CHECKPOINT_PATH = 'finetuned_openclip_v3'\n",
    "WEIGHT_DECAY = 1e-2\n",
    "SPLITS = ['train', 'val', 'test', 'обучение', 'валидация', 'тест']\n",
    "\n",
    "\n",
    "\n",
    " # можно адаптировать под свой CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "all_classes = sorted(df['class_name'].unique().tolist())\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "prompts = [f\"{c}\" for c in all_classes]\n",
    "text_tokens = tokenizer(prompts).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_f1s = []\n",
    "gzsl_seen_accs = []\n",
    "gzsl_unseen_accs = []\n",
    "gzsl_hmeans = []\n",
    "\n",
    "\n",
    "def compute_gzsl(trues_cls, preds_cls, seen_classes, unseen_classes):\n",
    "    seen_mask = [cls in seen_classes for cls in trues_cls]\n",
    "    unseen_mask = [cls in unseen_classes for cls in trues_cls]\n",
    "    seen_acc = sum([t == p for t, p, m in zip(\n",
    "        trues_cls, preds_cls, seen_mask) if m]) / max(sum(seen_mask), 1)\n",
    "    unseen_acc = sum([t == p for t, p, m in zip(trues_cls, preds_cls, unseen_mask) if m]) / max(sum(unseen_mask), 1)\n",
    "    h_mean = 2 * seen_acc * unseen_acc / (seen_acc + unseen_acc) if (seen_acc + unseen_acc) > 0 else 0.0\n",
    "    return seen_acc, unseen_acc, h_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUB_200_2011_split Test] Top-1 Accuracy: 0.7440, Macro F1: 0.7246\n",
      "[CUB_200_2011_split GZSL] Seen acc: 0.0000 | Unseen acc: 0.7440 | H-mean: 0.0000\n",
      "[dtd_split Test] Top-1 Accuracy: 0.6213, Macro F1: 0.6042\n",
      "[dtd_split GZSL] Seen acc: 0.0000 | Unseen acc: 0.6213 | H-mean: 0.0000\n",
      "[fungi_clef_2022_split Test] Top-1 Accuracy: 0.1873, Macro F1: 0.1245\n",
      "[fungi_clef_2022_split GZSL] Seen acc: 0.0000 | Unseen acc: 0.1873 | H-mean: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_domain(model, data_loader, text_tokens, class_names, seen_classes, unseen_classes, domain_name):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            logits = 100. * image_features @ text_features.T\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            trues.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    macro_f1 = f1_score(trues, preds, average='macro')\n",
    "    print(f\"[{domain_name} Test] Top-1 Accuracy: {acc:.4f}, Macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "    idx_to_class = {i: c for i, c in enumerate(class_names)}\n",
    "    trues_cls = [idx_to_class[i] for i in trues]\n",
    "    preds_cls = [idx_to_class[i] for i in preds]\n",
    "    seen_acc, unseen_acc, h_mean = compute_gzsl(trues_cls, preds_cls, seen_classes, unseen_classes)\n",
    "    print(f\"[{domain_name} GZSL] Seen acc: {seen_acc:.4f} | Unseen acc: {unseen_acc:.4f} | H-mean: {h_mean:.4f}\")\n",
    "    return acc, macro_f1, seen_acc, unseen_acc, h_mean\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = CsvImageDataset(CSV_PATH, split='test', transform=val_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "DATASET_DIRS = [\n",
    "    'CUB_200_2011_split',\n",
    "    'dtd_split',\n",
    "    'fungi_clef_2022_split'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "\n",
    "# Путь к своему чекпоинту\n",
    "checkpoint_path = \"finetuned_openclip_v3_epoch2_h0.44423542499502605.pth\"\n",
    "\n",
    "# Название архитектуры и процессора (см. как обучал, например, 'ViT-B-32')\n",
    "model_name = \"ViT-B-32\"\n",
    "pretrained = None  # None если используешь свой чекпоинт\n",
    "\n",
    "# Загружаем модель\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name,\n",
    "    pretrained=pretrained\n",
    ")\n",
    "# Загрузка весов\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"] if \"state_dict\" in checkpoint else checkpoint)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for domain in DATASET_DIRS:\n",
    "    # Фильтруем по нужному домену и 'test'\n",
    "    domain_df = df[(df['split'] == 'test') & (df['domain'] == domain)].reset_index(drop=True)\n",
    "    if len(domain_df) == 0:\n",
    "        continue\n",
    "    # Создаём датасет для этого домена\n",
    "    domain_dataset = CsvImageDataset(CSV_PATH, split='test', transform=val_transform)\n",
    "    # Переопределяем выборку: только текущий домен\n",
    "    domain_dataset.df = domain_df\n",
    "    # Если класс_names/мэппинг отличается, пересоздай их:\n",
    "    domain_dataset.class_names = sorted(domain_dataset.df['class_name'].unique())\n",
    "    domain_dataset.class_to_idx = {cls: i for i, cls in enumerate(domain_dataset.class_names)}\n",
    "    domain_loader = DataLoader(domain_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Для каждого домена пересчитай seen/unseen (на всякий случай)\n",
    "    train_classes = set(df[df['split'] == 'train']['class_name'].unique())\n",
    "    test_classes = set(domain_df['class_name'].unique())\n",
    "    unseen_classes = test_classes - train_classes\n",
    "    seen_classes = train_classes & test_classes\n",
    "\n",
    "    # Промпты (пересчитай для каждого подмножества классов!)\n",
    "    prompts = [f\"a photo of a {c.lower()}\" for c in domain_dataset.class_names]\n",
    "    text_tokens = tokenizer(prompts).to(DEVICE)\n",
    "\n",
    "    # Вызов функции\n",
    "    evaluate_on_domain(\n",
    "        model, domain_loader, text_tokens, domain_dataset.class_names,\n",
    "        seen_classes, unseen_classes, domain\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.logit_scale', 'module.visual.class_embedding', 'module.visual.positional_embedding', 'module.visual.proj', 'module.visual.conv1.weight', 'module.visual.ln_pre.weight', 'module.visual.ln_pre.bias', 'module.visual.transformer.resblocks.0.attn.in_proj_weight', 'module.visual.transformer.resblocks.0.attn.in_proj_bias', 'module.visual.transformer.resblocks.0.attn.out_proj.weight', 'module.visual.transformer.resblocks.0.attn.out_proj.bias', 'module.visual.transformer.resblocks.0.ln_1.weight', 'module.visual.transformer.resblocks.0.ln_1.bias', 'module.visual.transformer.resblocks.0.mlp.c_fc.weight', 'module.visual.transformer.resblocks.0.mlp.c_fc.bias', 'module.visual.transformer.resblocks.0.mlp.c_proj.weight', 'module.visual.transformer.resblocks.0.mlp.c_proj.bias', 'module.visual.transformer.resblocks.0.ln_2.weight', 'module.visual.transformer.resblocks.0.ln_2.bias', 'module.visual.transformer.resblocks.1.attn.in_proj_weight', 'module.visual.transformer.resblocks.1.attn.in_proj_bias', 'module.visual.transformer.resblocks.1.attn.out_proj.weight', 'module.visual.transformer.resblocks.1.attn.out_proj.bias', 'module.visual.transformer.resblocks.1.ln_1.weight', 'module.visual.transformer.resblocks.1.ln_1.bias', 'module.visual.transformer.resblocks.1.mlp.c_fc.weight', 'module.visual.transformer.resblocks.1.mlp.c_fc.bias', 'module.visual.transformer.resblocks.1.mlp.c_proj.weight', 'module.visual.transformer.resblocks.1.mlp.c_proj.bias', 'module.visual.transformer.resblocks.1.ln_2.weight', 'module.visual.transformer.resblocks.1.ln_2.bias', 'module.visual.transformer.resblocks.2.attn.in_proj_weight', 'module.visual.transformer.resblocks.2.attn.in_proj_bias', 'module.visual.transformer.resblocks.2.attn.out_proj.weight', 'module.visual.transformer.resblocks.2.attn.out_proj.bias', 'module.visual.transformer.resblocks.2.ln_1.weight', 'module.visual.transformer.resblocks.2.ln_1.bias', 'module.visual.transformer.resblocks.2.mlp.c_fc.weight', 'module.visual.transformer.resblocks.2.mlp.c_fc.bias', 'module.visual.transformer.resblocks.2.mlp.c_proj.weight', 'module.visual.transformer.resblocks.2.mlp.c_proj.bias', 'module.visual.transformer.resblocks.2.ln_2.weight', 'module.visual.transformer.resblocks.2.ln_2.bias', 'module.visual.transformer.resblocks.3.attn.in_proj_weight', 'module.visual.transformer.resblocks.3.attn.in_proj_bias', 'module.visual.transformer.resblocks.3.attn.out_proj.weight', 'module.visual.transformer.resblocks.3.attn.out_proj.bias', 'module.visual.transformer.resblocks.3.ln_1.weight', 'module.visual.transformer.resblocks.3.ln_1.bias', 'module.visual.transformer.resblocks.3.mlp.c_fc.weight', 'module.visual.transformer.resblocks.3.mlp.c_fc.bias', 'module.visual.transformer.resblocks.3.mlp.c_proj.weight', 'module.visual.transformer.resblocks.3.mlp.c_proj.bias', 'module.visual.transformer.resblocks.3.ln_2.weight', 'module.visual.transformer.resblocks.3.ln_2.bias', 'module.visual.transformer.resblocks.4.attn.in_proj_weight', 'module.visual.transformer.resblocks.4.attn.in_proj_bias', 'module.visual.transformer.resblocks.4.attn.out_proj.weight', 'module.visual.transformer.resblocks.4.attn.out_proj.bias', 'module.visual.transformer.resblocks.4.ln_1.weight', 'module.visual.transformer.resblocks.4.ln_1.bias', 'module.visual.transformer.resblocks.4.mlp.c_fc.weight', 'module.visual.transformer.resblocks.4.mlp.c_fc.bias', 'module.visual.transformer.resblocks.4.mlp.c_proj.weight', 'module.visual.transformer.resblocks.4.mlp.c_proj.bias', 'module.visual.transformer.resblocks.4.ln_2.weight', 'module.visual.transformer.resblocks.4.ln_2.bias', 'module.visual.transformer.resblocks.5.attn.in_proj_weight', 'module.visual.transformer.resblocks.5.attn.in_proj_bias', 'module.visual.transformer.resblocks.5.attn.out_proj.weight', 'module.visual.transformer.resblocks.5.attn.out_proj.bias', 'module.visual.transformer.resblocks.5.ln_1.weight', 'module.visual.transformer.resblocks.5.ln_1.bias', 'module.visual.transformer.resblocks.5.mlp.c_fc.weight', 'module.visual.transformer.resblocks.5.mlp.c_fc.bias', 'module.visual.transformer.resblocks.5.mlp.c_proj.weight', 'module.visual.transformer.resblocks.5.mlp.c_proj.bias', 'module.visual.transformer.resblocks.5.ln_2.weight', 'module.visual.transformer.resblocks.5.ln_2.bias', 'module.visual.transformer.resblocks.6.attn.in_proj_weight', 'module.visual.transformer.resblocks.6.attn.in_proj_bias', 'module.visual.transformer.resblocks.6.attn.out_proj.weight', 'module.visual.transformer.resblocks.6.attn.out_proj.bias', 'module.visual.transformer.resblocks.6.ln_1.weight', 'module.visual.transformer.resblocks.6.ln_1.bias', 'module.visual.transformer.resblocks.6.mlp.c_fc.weight', 'module.visual.transformer.resblocks.6.mlp.c_fc.bias', 'module.visual.transformer.resblocks.6.mlp.c_proj.weight', 'module.visual.transformer.resblocks.6.mlp.c_proj.bias', 'module.visual.transformer.resblocks.6.ln_2.weight', 'module.visual.transformer.resblocks.6.ln_2.bias', 'module.visual.transformer.resblocks.7.attn.in_proj_weight', 'module.visual.transformer.resblocks.7.attn.in_proj_bias', 'module.visual.transformer.resblocks.7.attn.out_proj.weight', 'module.visual.transformer.resblocks.7.attn.out_proj.bias', 'module.visual.transformer.resblocks.7.ln_1.weight', 'module.visual.transformer.resblocks.7.ln_1.bias', 'module.visual.transformer.resblocks.7.mlp.c_fc.weight', 'module.visual.transformer.resblocks.7.mlp.c_fc.bias', 'module.visual.transformer.resblocks.7.mlp.c_proj.weight', 'module.visual.transformer.resblocks.7.mlp.c_proj.bias', 'module.visual.transformer.resblocks.7.ln_2.weight', 'module.visual.transformer.resblocks.7.ln_2.bias', 'module.visual.transformer.resblocks.8.attn.in_proj_weight', 'module.visual.transformer.resblocks.8.attn.in_proj_bias', 'module.visual.transformer.resblocks.8.attn.out_proj.weight', 'module.visual.transformer.resblocks.8.attn.out_proj.bias', 'module.visual.transformer.resblocks.8.ln_1.weight', 'module.visual.transformer.resblocks.8.ln_1.bias', 'module.visual.transformer.resblocks.8.mlp.c_fc.weight', 'module.visual.transformer.resblocks.8.mlp.c_fc.bias', 'module.visual.transformer.resblocks.8.mlp.c_proj.weight', 'module.visual.transformer.resblocks.8.mlp.c_proj.bias', 'module.visual.transformer.resblocks.8.ln_2.weight', 'module.visual.transformer.resblocks.8.ln_2.bias', 'module.visual.transformer.resblocks.9.attn.in_proj_weight', 'module.visual.transformer.resblocks.9.attn.in_proj_bias', 'module.visual.transformer.resblocks.9.attn.out_proj.weight', 'module.visual.transformer.resblocks.9.attn.out_proj.bias', 'module.visual.transformer.resblocks.9.ln_1.weight', 'module.visual.transformer.resblocks.9.ln_1.bias', 'module.visual.transformer.resblocks.9.mlp.c_fc.weight', 'module.visual.transformer.resblocks.9.mlp.c_fc.bias', 'module.visual.transformer.resblocks.9.mlp.c_proj.weight', 'module.visual.transformer.resblocks.9.mlp.c_proj.bias', 'module.visual.transformer.resblocks.9.ln_2.weight', 'module.visual.transformer.resblocks.9.ln_2.bias', 'module.visual.transformer.resblocks.10.attn.in_proj_weight', 'module.visual.transformer.resblocks.10.attn.in_proj_bias', 'module.visual.transformer.resblocks.10.attn.out_proj.weight', 'module.visual.transformer.resblocks.10.attn.out_proj.bias', 'module.visual.transformer.resblocks.10.ln_1.weight', 'module.visual.transformer.resblocks.10.ln_1.bias', 'module.visual.transformer.resblocks.10.mlp.c_fc.weight', 'module.visual.transformer.resblocks.10.mlp.c_fc.bias', 'module.visual.transformer.resblocks.10.mlp.c_proj.weight', 'module.visual.transformer.resblocks.10.mlp.c_proj.bias', 'module.visual.transformer.resblocks.10.ln_2.weight', 'module.visual.transformer.resblocks.10.ln_2.bias', 'module.visual.transformer.resblocks.11.attn.in_proj_weight', 'module.visual.transformer.resblocks.11.attn.in_proj_bias', 'module.visual.transformer.resblocks.11.attn.out_proj.weight', 'module.visual.transformer.resblocks.11.attn.out_proj.bias', 'module.visual.transformer.resblocks.11.ln_1.weight', 'module.visual.transformer.resblocks.11.ln_1.bias', 'module.visual.transformer.resblocks.11.mlp.c_fc.weight', 'module.visual.transformer.resblocks.11.mlp.c_fc.bias', 'module.visual.transformer.resblocks.11.mlp.c_proj.weight', 'module.visual.transformer.resblocks.11.mlp.c_proj.bias', 'module.visual.transformer.resblocks.11.ln_2.weight', 'module.visual.transformer.resblocks.11.ln_2.bias', 'module.visual.ln_post.weight', 'module.visual.ln_post.bias', 'module.encode_text.positional_embedding', 'module.encode_text.transformer.resblocks.0.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.0.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.0.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.0.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.0.ln_1.weight', 'module.encode_text.transformer.resblocks.0.ln_1.bias', 'module.encode_text.transformer.resblocks.0.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.0.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.0.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.0.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.0.ln_2.weight', 'module.encode_text.transformer.resblocks.0.ln_2.bias', 'module.encode_text.transformer.resblocks.1.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.1.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.1.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.1.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.1.ln_1.weight', 'module.encode_text.transformer.resblocks.1.ln_1.bias', 'module.encode_text.transformer.resblocks.1.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.1.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.1.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.1.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.1.ln_2.weight', 'module.encode_text.transformer.resblocks.1.ln_2.bias', 'module.encode_text.transformer.resblocks.2.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.2.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.2.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.2.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.2.ln_1.weight', 'module.encode_text.transformer.resblocks.2.ln_1.bias', 'module.encode_text.transformer.resblocks.2.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.2.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.2.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.2.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.2.ln_2.weight', 'module.encode_text.transformer.resblocks.2.ln_2.bias', 'module.encode_text.transformer.resblocks.3.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.3.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.3.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.3.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.3.ln_1.weight', 'module.encode_text.transformer.resblocks.3.ln_1.bias', 'module.encode_text.transformer.resblocks.3.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.3.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.3.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.3.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.3.ln_2.weight', 'module.encode_text.transformer.resblocks.3.ln_2.bias', 'module.encode_text.transformer.resblocks.4.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.4.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.4.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.4.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.4.ln_1.weight', 'module.encode_text.transformer.resblocks.4.ln_1.bias', 'module.encode_text.transformer.resblocks.4.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.4.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.4.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.4.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.4.ln_2.weight', 'module.encode_text.transformer.resblocks.4.ln_2.bias', 'module.encode_text.transformer.resblocks.5.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.5.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.5.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.5.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.5.ln_1.weight', 'module.encode_text.transformer.resblocks.5.ln_1.bias', 'module.encode_text.transformer.resblocks.5.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.5.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.5.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.5.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.5.ln_2.weight', 'module.encode_text.transformer.resblocks.5.ln_2.bias', 'module.encode_text.transformer.resblocks.6.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.6.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.6.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.6.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.6.ln_1.weight', 'module.encode_text.transformer.resblocks.6.ln_1.bias', 'module.encode_text.transformer.resblocks.6.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.6.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.6.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.6.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.6.ln_2.weight', 'module.encode_text.transformer.resblocks.6.ln_2.bias', 'module.encode_text.transformer.resblocks.7.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.7.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.7.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.7.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.7.ln_1.weight', 'module.encode_text.transformer.resblocks.7.ln_1.bias', 'module.encode_text.transformer.resblocks.7.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.7.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.7.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.7.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.7.ln_2.weight', 'module.encode_text.transformer.resblocks.7.ln_2.bias', 'module.encode_text.transformer.resblocks.8.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.8.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.8.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.8.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.8.ln_1.weight', 'module.encode_text.transformer.resblocks.8.ln_1.bias', 'module.encode_text.transformer.resblocks.8.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.8.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.8.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.8.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.8.ln_2.weight', 'module.encode_text.transformer.resblocks.8.ln_2.bias', 'module.encode_text.transformer.resblocks.9.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.9.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.9.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.9.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.9.ln_1.weight', 'module.encode_text.transformer.resblocks.9.ln_1.bias', 'module.encode_text.transformer.resblocks.9.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.9.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.9.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.9.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.9.ln_2.weight', 'module.encode_text.transformer.resblocks.9.ln_2.bias', 'module.encode_text.transformer.resblocks.10.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.10.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.10.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.10.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.10.ln_1.weight', 'module.encode_text.transformer.resblocks.10.ln_1.bias', 'module.encode_text.transformer.resblocks.10.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.10.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.10.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.10.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.10.ln_2.weight', 'module.encode_text.transformer.resblocks.10.ln_2.bias', 'module.encode_text.transformer.resblocks.11.attn.in_proj_weight', 'module.encode_text.transformer.resblocks.11.attn.in_proj_bias', 'module.encode_text.transformer.resblocks.11.attn.out_proj.weight', 'module.encode_text.transformer.resblocks.11.attn.out_proj.bias', 'module.encode_text.transformer.resblocks.11.ln_1.weight', 'module.encode_text.transformer.resblocks.11.ln_1.bias', 'module.encode_text.transformer.resblocks.11.mlp.c_fc.weight', 'module.encode_text.transformer.resblocks.11.mlp.c_fc.bias', 'module.encode_text.transformer.resblocks.11.mlp.c_proj.weight', 'module.encode_text.transformer.resblocks.11.mlp.c_proj.bias', 'module.encode_text.transformer.resblocks.11.ln_2.weight', 'module.encode_text.transformer.resblocks.11.ln_2.bias', 'module.encode_text.token_embedding.weight', 'module.encode_text.ln_final.weight', 'module.encode_text.ln_final.bias', 'module.encode_text.text_projection.weight', 'module.encode_text.text_projection.bias', 'module.projector.linear1.weight', 'module.projector.linear1.bias', 'module.projector.bn1.weight', 'module.projector.bn1.bias', 'module.projector.bn1.running_mean', 'module.projector.bn1.running_var', 'module.projector.bn1.num_batches_tracked', 'module.projector.linear2.weight', 'module.projector.linear2.bias', 'module.projector.bn2.weight', 'module.projector.bn2.bias', 'module.projector.bn2.running_mean', 'module.projector.bn2.running_var', 'module.projector.bn2.num_batches_tracked', 'module.projector.linear3.weight', 'module.projector.linear3.bias', 'module.projector.bn3.weight', 'module.projector.bn3.bias', 'module.projector.bn3.running_mean', 'module.projector.bn3.running_var', 'module.projector.bn3.num_batches_tracked', 'module.predictor.linear1.weight', 'module.predictor.linear1.bias', 'module.predictor.bn1.weight', 'module.predictor.bn1.bias', 'module.predictor.bn1.running_mean', 'module.predictor.bn1.running_var', 'module.predictor.bn1.num_batches_tracked', 'module.predictor.layer2.weight', 'module.predictor.layer2.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuned_openclip_v3_epoch1_h0.3820202953874789\n",
    "\n",
    "[CUB_200_2011_split Test] Top-1 Accuracy: 0.7487, Macro F1: 0.7293\n",
    "[CUB_200_2011_split GZSL] Seen acc: 0.0000 | Unseen acc: 0.7487 | H-mean: 0.0000\n",
    "[dtd_split Test] Top-1 Accuracy: 0.6213, Macro F1: 0.6008\n",
    "[dtd_split GZSL] Seen acc: 0.0000 | Unseen acc: 0.6213 | H-mean: 0.0000\n",
    "[fungi_clef_2022_split Test] Top-1 Accuracy: 0.1770, Macro F1: 0.1113\n",
    "[fungi_clef_2022_split GZSL] Seen acc: 0.0000 | Unseen acc: 0.1770 | H-mean: 0.0000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
