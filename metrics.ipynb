{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeCLIP(\n",
      "  (visual_conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "  (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (visual_transformer): QuickTransformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (token_embedding): Embedding(49408, 512)\n",
      "  (text_transformer): QuickTransformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# ====== Минимальный вариант архитектуры из DeCLIP для ViT (из declip.py, сильно упрощён) ======\n",
    "\n",
    "class QuickTransformer(nn.Module):\n",
    "    def __init__(self, width, layers, heads):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=width, nhead=heads)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class DeCLIP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim=512, \n",
    "                 vision_width=768, \n",
    "                 vision_layers=12, \n",
    "                 vision_patch_size=16,\n",
    "                 image_resolution=224, \n",
    "                 text_width=512, \n",
    "                 text_layers=12, \n",
    "                 text_heads=8, \n",
    "                 vocab_size=49408):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_resolution = image_resolution\n",
    "        self.vision_patch_size = vision_patch_size\n",
    "        self.visual_conv1 = nn.Conv2d(3, vision_width, kernel_size=vision_patch_size, stride=vision_patch_size, bias=False)\n",
    "        scale = vision_width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(vision_width))\n",
    "        num_patches = (image_resolution // vision_patch_size) ** 2\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn(num_patches + 1, vision_width))\n",
    "        self.ln_pre = nn.LayerNorm(vision_width)\n",
    "        self.visual_transformer = QuickTransformer(vision_width, vision_layers, heads=12)\n",
    "        self.ln_post = nn.LayerNorm(vision_width)\n",
    "        self.visual_projection = nn.Parameter(torch.randn(vision_width, embed_dim))\n",
    "\n",
    "        # Текстовый энкодер (очень упрощён)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, text_width)\n",
    "        self.positional_embedding_text = nn.Parameter(torch.empty(77, text_width))\n",
    "        self.text_transformer = QuickTransformer(text_width, text_layers, text_heads)\n",
    "        self.ln_final = nn.LayerNorm(text_width)\n",
    "        self.text_projection = nn.Parameter(torch.randn(text_width, embed_dim))\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1 / 0.07)))\n",
    "        \n",
    "    def encode_image(self, image):\n",
    "        # image: [B, 3, H, W]\n",
    "        x = self.visual_conv1(image)   # [B, C, H', W']\n",
    "        x = x.flatten(2).permute(2, 0, 1)  # [N, B, C]\n",
    "        class_emb = self.class_embedding.unsqueeze(0).unsqueeze(1).expand(1, x.size(1), -1)\n",
    "        x = torch.cat([class_emb, x], dim=0)\n",
    "        x = x + self.positional_embedding[:x.size(0), :].unsqueeze(1)\n",
    "        x = self.ln_pre(x)\n",
    "        x = self.visual_transformer(x)\n",
    "        x = x[0]\n",
    "        x = self.ln_post(x)\n",
    "        x = x @ self.visual_projection\n",
    "        x = x / x.norm(dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        # text: [B, seq_len]\n",
    "        x = self.token_embedding(text) + self.positional_embedding_text[:text.size(1), :]\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch, dim]\n",
    "        x = self.text_transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # [batch, seq_len, dim]\n",
    "        x = self.ln_final(x)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        x = x / x.norm(dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "        return logits\n",
    "\n",
    "# =================== DeCLIP Inference Wrapper ===================\n",
    "\n",
    "class DeCLIPInference:\n",
    "    def __init__(self, model_ckpt_path, classnames, device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Обычно параметры можно взять из документации к весам/статье\n",
    "        self.model = DeCLIP(\n",
    "            embed_dim=512, vision_width=768, vision_layers=12, \n",
    "            vision_patch_size=16, image_resolution=224,\n",
    "            text_width=512, text_layers=12, text_heads=8, vocab_size=49408\n",
    "        ).to(self.device)\n",
    "        print(self.model)\n",
    "        # Загрузка весов с удалением префикса 'module.'\n",
    "        state_dict = torch.load(model_ckpt_path, map_location=self.device)\n",
    "        if \"model\" in state_dict:\n",
    "            state_dict = state_dict[\"model\"]\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        self.model.load_state_dict(new_state_dict, strict=False)\n",
    "        self.model.eval()\n",
    "        # Препроцессинг (CLIP-style)\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                std=[0.26862954, 0.26130258, 0.27577711]\n",
    "            ),\n",
    "        ])\n",
    "        # Классы и токенизация\n",
    "        import clip  # нужен clip.tokenize (или возьми токенизацию из DeCLIP!)\n",
    "        self.classnames = classnames\n",
    "        with torch.no_grad():\n",
    "            text_tokens = clip.tokenize([f\"a photo of a {c}\" for c in classnames]).to(self.device)\n",
    "            self.text_features = self.model.encode_text(text_tokens)\n",
    "            self.text_features = self.text_features / self.text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def predict(self, image_paths, batch_size=16):\n",
    "        preds = []\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = [self.preprocess(Image.open(p).convert(\"RGB\")) for p in batch_paths]\n",
    "            images = torch.stack(images).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.encode_image(images)\n",
    "                logits = (self.model.logit_scale.exp() * image_features @ self.text_features.t()).softmax(dim=-1)\n",
    "                pred_indices = logits.argmax(dim=-1).cpu().tolist()\n",
    "                batch_preds = [self.classnames[i] for i in pred_indices]\n",
    "                preds.extend(batch_preds)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        return preds\n",
    "\n",
    "    def predict_proba(self, image_paths, topk=5, batch_size=16):\n",
    "        topk_preds = []\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = [self.preprocess(Image.open(p).convert(\"RGB\")) for p in batch_paths]\n",
    "            images = torch.stack(images).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.encode_image(images)\n",
    "                logits = (self.model.logit_scale.exp() * image_features @ self.text_features.t()).softmax(dim=-1)\n",
    "                probs = logits.cpu().tolist()\n",
    "                for p in probs:\n",
    "                    topk_indices = sorted(range(len(p)), key=lambda i: p[i], reverse=True)[:topk]\n",
    "                    topk_labels = [self.classnames[i] for i in topk_indices]\n",
    "                    topk_preds.append(topk_labels)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        return topk_preds\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"DeCLIP-original\"\n",
    "    \n",
    "# =========== Пример использования ===========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classnames = [\"cat\", \"dog\", \"car\"]\n",
    "    model_ckpt_path = \"vitb32.pth.tar\"\n",
    "    #model = DeCLIPInference(model_ckpt_path, classnames, device=\"cuda\")\n",
    "    new_state_dict = DeCLIPInference(model_ckpt_path=model_ckpt_path, classnames=classnames, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys = set(model.state_dict().keys())\n",
    "ckpt_keys = set(new_state_dict.keys())\n",
    "print(\"Не совпало:\", ckpt_keys - model_keys)\n",
    "print(\"Отсутствуют в ckpt:\", model_keys - ckpt_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eb9fbb97494b6191b601fb4f272bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: person\n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\").to(\"cuda\")\n",
    "st_model = SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "\n",
    "classnames = [\"cat\", \"dog\", \"car\", \"tree\", \"person\"]\n",
    "prompts = [f\"a photo of a {c}\" for c in classnames]\n",
    "class_embeds = st_model.encode(prompts, normalize_embeddings=True)\n",
    "\n",
    "img = Image.open(\"img1.jpg\").convert(\"RGB\")\n",
    "inputs = processor(images=img, text=\"Describe this image\", return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=15)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True).lower()\n",
    "caption_embed = st_model.encode([caption], normalize_embeddings=True)\n",
    "sims = (caption_embed @ class_embeds.T).squeeze(0)\n",
    "pred_idx = int(sims.argmax())\n",
    "print(\"Predicted class:\", classnames[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_classes(all_classes, n_train, n_test, seed=42):\n",
    "    random.seed(seed)\n",
    "    shuffled = list(all_classes)\n",
    "    random.shuffle(shuffled)\n",
    "    train_classes = shuffled[:n_train]\n",
    "    test_classes = shuffled[n_train:n_train+n_test]\n",
    "    return train_classes, test_classes\n",
    "\n",
    "def split_dataset_by_class(\n",
    "    source_dir,\n",
    "    output_dir,\n",
    "    n_train_classes,\n",
    "    n_test_classes,\n",
    "    n_train_images_per_class,\n",
    "    n_test_images_per_class,\n",
    "    seed=42\n",
    "):\n",
    "    # Получаем все классы (папки) в датасете\n",
    "    all_classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "    train_classes, test_classes = split_classes(all_classes, n_train_classes, n_test_classes, seed)\n",
    "    \n",
    "    print(f\"Train classes: {train_classes}\")\n",
    "    print(f\"Test classes: {test_classes}\")\n",
    "    \n",
    "    for split, splited_classes, n_images in [('train', train_classes, n_train_images_per_class),\n",
    "                                           ('test', test_classes, n_test_images_per_class)]:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        for class_name in splited_classes:\n",
    "            src_class_dir = os.path.join(source_dir, class_name)\n",
    "            dst_class_dir = os.path.join(split_dir, class_name)\n",
    "            os.makedirs(dst_class_dir, exist_ok=True)\n",
    "            images = os.listdir(src_class_dir)\n",
    "            random.seed(seed)  # Для стабильности отбора изображений — фиксируем seed\n",
    "            random.shuffle(images)\n",
    "            selected = images[:n_images] if n_images else images\n",
    "            for img in selected:\n",
    "                shutil.copy(os.path.join(src_class_dir, img), os.path.join(dst_class_dir, img))\n",
    "\n",
    "# ---------- FungiCLEF ----------\n",
    "def split_fungi_clef_dataset(\n",
    "    source_dir,\n",
    "    csv_file,\n",
    "    output_dir,\n",
    "    n_train_classes,\n",
    "    n_test_classes,\n",
    "    n_train_images_per_class,\n",
    "    n_test_images_per_class,\n",
    "    seed=42\n",
    "):\n",
    "    import csv\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Загружаем метаданные из CSV\n",
    "    samples_by_class = defaultdict(list)\n",
    "    with open(os.path.join(source_dir, csv_file), newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            class_name = \" \".join(row[\"scientificName\"].split()[:2])  # Берем только род и вид\n",
    "            img_path = os.path.join(source_dir, \"DF20_300\", row[\"image_path\"])\n",
    "            if os.path.isfile(img_path):\n",
    "                samples_by_class[class_name].append(img_path)\n",
    "    \n",
    "    # Получаем список всех классов\n",
    "    all_classes = sorted(samples_by_class.keys())\n",
    "    train_classes, test_classes = split_classes(all_classes, n_train_classes, n_test_classes, seed)\n",
    "    \n",
    "    print(f\"Train classes: {train_classes}\")\n",
    "    print(f\"Test classes: {test_classes}\")\n",
    "    \n",
    "    for split, splited_classes, n_images in [('train', train_classes, n_train_images_per_class),\n",
    "                                           ('test', test_classes, n_test_images_per_class)]:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        for class_name in splited_classes:\n",
    "            dst_class_dir = os.path.join(split_dir, class_name)\n",
    "            os.makedirs(dst_class_dir, exist_ok=True)\n",
    "            \n",
    "            # Получаем все изображения для класса\n",
    "            images = samples_by_class[class_name]\n",
    "            random.seed(seed)\n",
    "            random.shuffle(images)\n",
    "            selected = images[:n_images] if n_images else images\n",
    "            \n",
    "            for img_path in selected:\n",
    "                img_name = os.path.basename(img_path)\n",
    "                dst_path = os.path.join(dst_class_dir, img_name)\n",
    "                try:\n",
    "                    shutil.copy(img_path, dst_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {img_path}: {e}\")\n",
    "\n",
    "# # Example usage:\n",
    "# split_fungi_clef_dataset(\n",
    "#     source_dir='fungi_clef_2022',\n",
    "#     csv_file='DF20-train_metadata.csv',\n",
    "#     output_dir='fungi_clef_2022_split',\n",
    "#     n_train_classes=120,\n",
    "#     n_test_classes=20,\n",
    "#     n_train_images_per_class=250,\n",
    "#     n_test_images_per_class=250,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # ---------- DTD ----------\n",
    "# split_dataset_by_class(\n",
    "#     source_dir='dtd/images/',             # замените на свой путь\n",
    "#     output_dir='dtd_split/',       # куда сохранить\n",
    "#     n_train_classes=29,\n",
    "#     n_test_classes=18,\n",
    "#     n_train_images_per_class=120,\n",
    "#     n_test_images_per_class=120,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# ---------- FungiCLEF ----------\n",
    "\n",
    "# # ---------- CUB_200_2011 ----------\n",
    "# split_dataset_by_class(\n",
    "#     source_dir='CUB_200_2011/CUB_200_2011/images',\n",
    "#     output_dir='CUB_200_2011_split/',\n",
    "#     n_train_classes=130,\n",
    "#     n_test_classes=70,\n",
    "#     n_train_images_per_class=58,    # ~7500/130\n",
    "#     n_test_images_per_class=59,     # ~4100/70\n",
    "#     seed=42\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Readable: True\n",
      "Writable: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"dtd/images/pleated\"\n",
    "print(\"Exists:\", os.path.exists(path))\n",
    "print(\"Readable:\", os.access(path, os.R_OK))\n",
    "print(\"Writable:\", os.access(path, os.W_OK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: ['067.Anna_Hummingbird', '188.Pileated_Woodpecker', '102.Western_Wood_Pewee', '194.Cactus_Wren', '112.Great_Grey_Shrike', '122.Harris_Sparrow', '014.Indigo_Bunting', '003.Sooty_Albatross', '065.Slaty_backed_Gull', '045.Northern_Fulmar', '137.Cliff_Swallow', '171.Myrtle_Warbler', '129.Song_Sparrow', '077.Tropical_Kingbird', '159.Black_and_white_Warbler', '168.Kentucky_Warbler', '046.Gadwall', '131.Vesper_Sparrow', '031.Black_billed_Cuckoo', '004.Groove_billed_Ani', '160.Black_throated_Blue_Warbler', '016.Painted_Bunting', '043.Yellow_bellied_Flycatcher', '127.Savannah_Sparrow', '187.American_Three_toed_Woodpecker', '163.Cape_May_Warbler', '048.European_Goldfinch', '111.Loggerhead_Shrike', '006.Least_Auklet', '195.Carolina_Wren', '038.Great_Crested_Flycatcher', '123.Henslow_Sparrow', '013.Bobolink', '037.Acadian_Flycatcher', '154.Red_eyed_Vireo', '070.Green_Violetear', '062.Herring_Gull', '196.House_Wren', '033.Yellow_billed_Cuckoo', '173.Orange_crowned_Warbler', '172.Nashville_Warbler', '061.Heermann_Gull', '011.Rusty_Blackbird', '145.Elegant_Tern', '161.Blue_winged_Warbler', '053.Western_Grebe', '126.Nelson_Sharp_tailed_Sparrow', '107.Common_Raven', '074.Florida_Jay', '114.Black_throated_Sparrow', '106.Horned_Puffin', '096.Hooded_Oriole', '104.American_Pipit', '146.Forsters_Tern', '044.Frigatebird', '001.Black_footed_Albatross', '162.Canada_Warbler', '193.Bewick_Wren', '017.Cardinal', '081.Pied_Kingfisher', '103.Sayornis', '039.Least_Flycatcher', '157.Yellow_throated_Vireo', '177.Prothonotary_Warbler', '136.Barn_Swallow', '079.Belted_Kingfisher', '116.Chipping_Sparrow', '020.Yellow_breasted_Chat', '015.Lazuli_Bunting', '178.Swainson_Warbler', '147.Least_Tern', '180.Wilson_Warbler', '066.Western_Gull', '085.Horned_Lark', '166.Golden_winged_Warbler', '047.American_Goldfinch', '105.Whip_poor_Will', '080.Green_Kingfisher', '055.Evening_Grosbeak', '197.Marsh_Wren', '086.Pacific_Loon', '119.Field_Sparrow', '110.Geococcyx', '134.Cape_Glossy_Starling', '143.Caspian_Tern', '034.Gray_crowned_Rosy_Finch', '019.Gray_Catbird', '141.Artic_Tern', '124.Le_Conte_Sparrow', '176.Prairie_Warbler', '064.Ring_billed_Gull', '084.Red_legged_Kittiwake', '149.Brown_Thrasher', '101.White_Pelican', '156.White_eyed_Vireo', '073.Blue_Jay', '028.Brown_Creeper', '185.Bohemian_Waxwing', '113.Baird_Sparrow', '052.Pied_billed_Grebe', '175.Pine_Warbler', '005.Crested_Auklet', '030.Fish_Crow', '184.Louisiana_Waterthrush', '100.Brown_Pelican', '099.Ovenbird', '132.White_crowned_Sparrow', '125.Lincoln_Sparrow', '199.Winter_Wren', '135.Bank_Swallow', '179.Tennessee_Warbler', '121.Grasshopper_Sparrow', '035.Purple_Finch', '049.Boat_tailed_Grackle', '170.Mourning_Warbler', '139.Scarlet_Tanager', '150.Sage_Thrasher', '133.White_throated_Sparrow', '128.Seaside_Sparrow', '022.Chuck_will_Widow', '082.Ringed_Kingfisher', '078.Gray_Kingbird', '010.Red_winged_Blackbird', '083.White_breasted_Kingfisher', '165.Chestnut_sided_Warbler', '120.Fox_Sparrow', '090.Red_breasted_Merganser', '069.Rufous_Hummingbird', '054.Blue_Grosbeak', '091.Mockingbird']\n",
      "Test classes: ['095.Baltimore_Oriole', '042.Vermilion_Flycatcher', '094.White_breasted_Nuthatch', '117.Clay_colored_Sparrow', '200.Common_Yellowthroat', '158.Bay_breasted_Warbler', '026.Bronzed_Cowbird', '181.Worm_eating_Warbler', '148.Green_tailed_Towhee', '075.Green_Jay', '059.California_Gull', '153.Philadelphia_Vireo', '018.Spotted_Catbird', '050.Eared_Grebe', '093.Clark_Nutcracker', '076.Dark_eyed_Junco', '142.Black_Tern', '021.Eastern_Towhee', '097.Orchard_Oriole', '032.Mangrove_Cuckoo', '138.Tree_Swallow', '118.House_Sparrow', '012.Yellow_headed_Blackbird', '068.Ruby_throated_Hummingbird', '089.Hooded_Merganser', '092.Nighthawk', '025.Pelagic_Cormorant', '098.Scott_Oriole', '183.Northern_Waterthrush', '191.Red_headed_Woodpecker', '087.Mallard', '182.Yellow_Warbler', '040.Olive_sided_Flycatcher', '169.Magnolia_Warbler', '088.Western_Meadowlark', '186.Cedar_Waxwing', '041.Scissor_tailed_Flycatcher', '002.Laysan_Albatross', '072.Pomarine_Jaeger', '151.Black_capped_Vireo', '115.Brewer_Sparrow', '057.Rose_breasted_Grosbeak', '108.White_necked_Raven', '192.Downy_Woodpecker', '167.Hooded_Warbler', '051.Horned_Grebe', '144.Common_Tern', '198.Rock_Wren', '155.Warbling_Vireo', '130.Tree_Sparrow', '060.Glaucous_winged_Gull', '056.Pine_Grosbeak', '024.Red_faced_Cormorant', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '109.American_Redstart', '152.Blue_headed_Vireo', '023.Brandt_Cormorant', '140.Summer_Tanager', '174.Palm_Warbler', '027.Shiny_Cowbird', '189.Red_bellied_Woodpecker', '036.Northern_Flicker', '058.Pigeon_Guillemot', '063.Ivory_Gull', '071.Long_tailed_Jaeger', '190.Red_cockaded_Woodpecker', '007.Parakeet_Auklet', '029.American_Crow', '164.Cerulean_Warbler']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_train_val_with_unseen(\n",
    "    train_dir,\n",
    "    test_dir,\n",
    "    output_dir,\n",
    "    validation_ratio=0.2,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Создает валидационную выборку, включающую:\n",
    "    - часть изображений из train (seen classes)\n",
    "    - все изображения из test (unseen classes)\n",
    "\n",
    "    train_dir: путь к папке с обучающими seen-классами\n",
    "    test_dir: путь к папке с unseen-классами\n",
    "    output_dir: папка, где будет создана структура val/\n",
    "    \"\"\"\n",
    "    val_dir = os.path.join(output_dir, 'val')\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "    # 1. seen-классы: берем часть изображений\n",
    "    seen_classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "\n",
    "    for class_name in seen_classes:\n",
    "        src_class_dir = os.path.join(train_dir, class_name)\n",
    "        val_class_dir = os.path.join(val_dir, class_name)\n",
    "        os.makedirs(val_class_dir, exist_ok=True)\n",
    "\n",
    "        images = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        random.seed(seed)\n",
    "        random.shuffle(images)\n",
    "        n_val = int(len(images) * validation_ratio)\n",
    "        val_images = images[:n_val]\n",
    "\n",
    "        for img in val_images:\n",
    "            shutil.copy(\n",
    "                os.path.join(src_class_dir, img),\n",
    "                os.path.join(val_class_dir, img)\n",
    "            )\n",
    "\n",
    "    # 2. unseen-классы: копируем все изображения\n",
    "    unseen_classes = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]\n",
    "\n",
    "    for class_name in unseen_classes:\n",
    "        src_class_dir = os.path.join(test_dir, class_name)\n",
    "        val_class_dir = os.path.join(val_dir, class_name)\n",
    "        os.makedirs(val_class_dir, exist_ok=True)\n",
    "\n",
    "        images = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        for img in images:\n",
    "            shutil.copy(\n",
    "                os.path.join(src_class_dir, img),\n",
    "                os.path.join(val_class_dir, img)\n",
    "            )\n",
    "                \n",
    "# ---------- DTD ----------\n",
    "# split_dataset_by_class(\n",
    "#     source_dir='dtd/images/',\n",
    "#     output_dir='dtd_split/',\n",
    "#     n_train_classes=29,\n",
    "#     n_test_classes=18,\n",
    "#     n_train_images_per_class=120,\n",
    "#     n_test_images_per_class=120,\n",
    "#     seed=42\n",
    "# )\n",
    "# split_train_val_with_unseen(\n",
    "#     train_dir='dtd_split/train',\n",
    "#     test_dir='dtd_split/test',\n",
    "#     output_dir='dtd_split',\n",
    "#     validation_ratio=0.2,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # ---------- FungiCLEF ----------\n",
    "# split_fungi_clef_dataset(\n",
    "#     source_dir='fungi_clef_2022',\n",
    "#     csv_file='DF20-train_metadata.csv',\n",
    "#     output_dir='fungi_clef_2022_split',\n",
    "#     n_train_classes=120,\n",
    "#     n_test_classes=20,\n",
    "#     n_train_images_per_class=250,\n",
    "#     n_test_images_per_class=250,\n",
    "#     seed=42\n",
    "#)\n",
    "split_train_val_with_unseen(\n",
    "    train_dir='fungi_clef_2022_split/train',\n",
    "    test_dir='fungi_clef_2022_split/test',\n",
    "    output_dir='fungi_clef_2022_split',\n",
    "    validation_ratio=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# ---------- CUB_200_2011 ----------\n",
    "split_dataset_by_class(\n",
    "    source_dir='CUB_200_2011/CUB_200_2011/images',\n",
    "    output_dir='CUB_200_2011_split/',\n",
    "    n_train_classes=130,\n",
    "    n_test_classes=70,\n",
    "    n_train_images_per_class=58,\n",
    "    n_test_images_per_class=59,\n",
    "    seed=42\n",
    ")\n",
    "split_train_val_with_unseen(\n",
    "    train_dir='CUB_200_2011_split/train',\n",
    "    test_dir='CUB_200_2011_split/test',\n",
    "    output_dir='CUB_200_2011_split',\n",
    "    validation_ratio=0.2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
